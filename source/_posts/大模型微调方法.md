---
title: 大模型微调方法
mathjax: true
toc: true
date: 2023-07-18 23:57:18
categories:
- NLP
tags:
- LLM
- PEFT
---

下面是一些参数高效的微调大模型方法：

<!--more-->

## LORA

## Prefix-Tuning
该方法主要用来做NLG任务（Table-to-text Generation、 Summarization），在输入token之前构造一段任务相关的virtual tokens作为Prefix，然后训练的时候只更新Prefix部分的参数，而大模型参数冻结。

#### 模型总览
![prefix tuning](https://cdn.staticaly.com/gh/TransformersWsz/image_hosting@master/image.6yre5xri92o0.png)

Prefix tokens初始化如下：
![init](https://cdn.staticaly.com/gh/TransformersWsz/image_hosting@master/image.6ahf2df1ub4.webp)

需要注意的是，在低资源场景下，用任务相关的单词来初始化prefix tokens，效果更好：
![words](https://cdn.staticaly.com/gh/TransformersWsz/image_hosting@master/image.3mxl4q9bqao0.webp)

## Prompt-tuning
Prompt-Tunning算是prefix-Tunning的简化版本，面向NLU任务，进行了更全面的效果对比，并且在大模型上成功打平了LM微调的效果。

#### 模型总览
![prompt tuning](https://cdn.staticaly.com/gh/TransformersWsz/image_hosting@master/image.3g21qtqft6w0.webp)

- 初始化：Prompt-tuning在输入层前置多个可训练的tokens，固定住大模型参数。实验结果表明用类标签来初始化prompts效果最好。
- prompt ensembling：针对同一个任务，构造多个不同的prompts，就相当于训练了多个模型。
  
![ensemble](https://cdn.staticaly.com/gh/TransformersWsz/image_hosting@master/image.3ygm24zc3f40.webp)

#### Prompt-tuning 与 Prefix-Tuning 不同
- 两者的基座模型不同，一个是T5，一个是BART和GPT2
- 前者关注NLU，后者关注NLG
- 前者参数更少，只需微调embeding层；后者需要微调所有层embedding，以及需要在输入层之后接一个MLP来稳定训练

## P-tuning V1 & P-tuning V2
P-tuning主要用GPT来做NLU任务，达到甚至超过BERT同等水平。

![v1](https://cdn.staticaly.com/gh/TransformersWsz/image_hosting@master/image.4fbg2k4h7wq0.webp)

v1做了如下两点优化：
- 考虑到预训练模型本身的embedding就比较离散了（随机初始化+梯度传回来小，最后只是小范围优化），同时prompt本身也是互相关联的，所以作者先用LSTM对prompt进行编码。
- 在prompt模板中，加入一些anchor tokens效果会更好。

v2主要是
___

## 参考
- [解密Prompt系列3. 冻结LM微调Prompt: Prefix-Tuning & Prompt-Tuning & P-Tuning](https://www.cnblogs.com/gogoSandy/p/17202169.html)
- [Prompt范式第二阶段｜Prefix-tuning、P-tuning、Prompt-tuning](https://zhuanlan.zhihu.com/p/400790006)
---
title: 大模型微调方法
mathjax: true
toc: true
date: 2023-07-18 23:57:18
categories:
- NLP
tags:
- LLM
- PEFT
---

下面是一些参数高效的微调大模型方法：

<!--more-->

## LORA

## Prefix-Tuning
该方法主要用来做NLG任务（Table-to-text Generation、 Summarization），在输入token之前构造一段任务相关的virtual tokens作为Prefix，然后训练的时候只更新Prefix部分的参数，而大模型参数冻结。

#### 模型总览
![prefix tuning](https://cdn.staticaly.com/gh/TransformersWsz/image_hosting@master/image.6yre5xri92o0.png)

Prefix tokens初始化如下：
![init](https://cdn.staticaly.com/gh/TransformersWsz/image_hosting@master/image.6ahf2df1ub4.webp)

需要注意的是，在低资源场景下，用任务相关的单词来初始化prefix tokens，效果更好：
![words](https://cdn.staticaly.com/gh/TransformersWsz/image_hosting@master/image.3mxl4q9bqao0.webp)

## Prompt-tuning
Prompt-Tunning算是prefix-Tunning的简化版本，面向NLU任务，进行了更全面的效果对比，并且在大模型上成功打平了LM微调的效果~


___

## 参考
- [解密Prompt系列3. 冻结LM微调Prompt: Prefix-Tuning & Prompt-Tuning & P-Tuning](https://www.cnblogs.com/gogoSandy/p/17202169.html)
- [Prompt范式第二阶段｜Prefix-tuning、P-tuning、Prompt-tuning](https://zhuanlan.zhihu.com/p/400790006)
---
title: 反向传播
date: 2019-05-29 21:56:21
mathjax: true
categories: 
- Machine Learning
tags:
- Neural Networks
- Algorithm
---

误差反向传播算法简称反向传播算法(Back Propagation)。使用反向传播算法的多层感知器又称为BP神经网络。

<!--more-->

BP算法是一个迭代算法，它的基本思想如下：

1. 将训练集数据输入到神经网络的输入层，经过隐藏层，最后达到输出层并输出结果，这就是前向传播过程。
2. 由于神经网络的输出结果与实际结果有误差，则计算估计值与实际值之间的误差，并将该误差从输出层向隐藏层反向传播，直至传播到输入层；
3. 在反向传播的过程中，根据误差调整各种参数的值（相连神经元的权重），使得总损失函数减小。
4. 迭代上述三个步骤（即对数据进行反复训练），直到满足停止准则。

# 示例

有如下一个神经网络：

![1.png](https://cdn.staticaly.com/gh/TransformersWsz/image_hosting@master/20190518224358906.1bsruizweubk.webp)

第一层是输入层，包含两个神经元 $i_1$，$i_2$ 和偏置项 $b_1$；第二层是隐藏层，包含两个神经元 $h_1$，$h_2$ 和偏置项 $b_2$；第三层是输出 $o_1$，$o_2$。每条线上标的 $w_i$ 是层与层之间连接的权重。激活函数是 $sigmod$ 函数。我们用 $z$ 表示某神经元的加权输入和；用 $a$ 表示某神经元的输出。

上述各参数赋值如下：

| 参数  |  值  |
| :---: | :--: |
| $i_1$ | 0.05 |
| $i_2$ | 0.10 |
| $w_1$ | 0.15 |
| $w_2$ | 0.20 |
| $w_3$ | 0.25 |
| $w_4$ | 0.30 |
| $w_5$ | 0.40 |
| $w_6$ | 0.45 |
| $w_7$ | 0.50 |
| $w_8$ | 0.55 |
| $b_1$ | 0.35 |
| $b_2$ | 0.60 |
| $o_1$ | 0.01 |
| $o_2$ | 0.99 |

## Step 1 前向传播

### 输入层 ---> 隐藏层

神经元 $h_1$ 的输入加权和：

$$
\begin{aligned}
   z_{h 1} & =w_1 * i_1+w_2 * i_2+b_1 * 1 \\
   & =0.15 * 0.05+0.2 * 0.1+0.35 * 1 \\
   & =0.3775
\end{aligned}
$$

神经元 $h_1$ 的输出 $a_{h1}$ ：
$$
a_{h1} = \frac{1}{1+e^{-z_{h1}}} = \frac{1}{1+e^{-0.3775}} = 0.593269992
$$
同理可得，神经元 $h_2$ 的输出 $a_{h2}$ ：
$$
a_{h2} = 0.596884378
$$

### 隐藏层 ---> 输出层

计算输出层神经元 $o1$ 和 $o2$ 的值：

$$
\begin{aligned}
   z_{o 1} & =w_5 * a_{h 1}+w_6 * a_{h 2}+b_2 * 1 \\
   & =0.4 * 0.593269992+0.45 * 0.596884378+0.6 * 1 \\
   & =1.105905967 \\
   a_{o 1} & =\frac{1}{1+e^{-z_{o 1}}} \\
   & =\frac{1}{1+e^{-1.105905967}} \\
   & =0.751365069 \\
   a_{o 2} & =0.772928465
\end{aligned}
$$

前向传播的过程就结束了，我们得到的输出值是 $[0.751365069, 0.772928465]$ ，与实际值 $[0.01, 0.99]$ 相差还很远。接下来我们对误差进行反向传播，更新权值，重新计算输出。

## Step 2 反向传播

1. 计算损失函数：

$$
E_{total} = \sum\frac{1}{2}(target - output)^2
$$

但是有两个输出，所以分别计算 $o_1$ 和 $o_2$ 的损失值，总误差为两者之和：
$$
E_{o_1} = \frac {1}{2}(0.01 - 0.751365069)^2 = 0.274811083 \\
E_{o_2} = \frac {1}{2}(0.99 - 0.772928465)^2 = 0.023560026 \\
E_{total} = E_{o_1} + E_{o_2} = 0.274811083 + 0.023560026 = 0.298371109
$$

2. 隐藏层 ---> 输出层的权值更新

以权重参数 $w_5$ 为例，如果我们想知道 $w_5$ 对整体损失产生了多少影响，可以用整体损失对 $w_5$ 求偏导：
$$
\frac{\partial E_{total}}{\partial w_5} = {\frac {\partial E_{total}}{\partial a_{o_1}}}*{\frac {\partial a_{o_1}}{\partial z_{o_1}} }*{ \frac {\partial z_{o_1}} {\partial w_5} }
$$
下面的图可以更直观了解误差是如何反向传播的：

![2.png](https://cdn.staticaly.com/gh/TransformersWsz/image_hosting@master/20190518224659584.31gxu503n220.webp)

我们现在分别来计算每个式子的值：

计算 $\frac {\partial E_{total}} {\partial a_{o_1}}$ ：
$$
E_{total} = \frac {1}{2}(target_{o_1} - a_{o_1})^2 + \frac {1}{2}(target_{o_2} - a_{o_1})^2 \\
\frac {\partial E_{total}} {\partial a_{o_1}} = 2 * \frac {1}{2} (target_{o_1} - a_{o_1})*-1 \\
\frac {\partial E_{total}} {\partial a_{o_1}} = -(target_{o_1} - a_{o_1}) = 0.751365069-0.01=0.741365069 \\
$$
计算 $\frac {\partial E_{total}} {\partial a_{o_1}}$ ：
$$
a_{o_1} = \frac {1}{1+e^{-z_{o_1}}} \\
\frac {\partial a_{o_1}} {\partial z_{o_1}} = a_{o_1}*(1-a_{o_1}) = 0.751365069*(1-0.751365069) = 0.186815602
$$
计算 $\frac {\partial z_{o_1}} {\partial w_5}$ ：
$$
z_{o_1} = w_5*a_{h1} + w_6*a_{h2} + b_2*1 \\
\frac {\partial z_{o_1}} {\partial w_5} = a_{h_1} = 0.593269992
$$
最后三者相乘：
$$
\frac {\partial E_{total}} {\partial w_5} = 0.741365069*0.186815602*0.593269992 = 0.082167041
$$
这样我们就算出整体损失 $E_{total}$ 对 $w_5$ 的偏导值。
$$
\frac {\partial E_{total}} {\partial w_5} = -(target_{o_1} - a_{o_1}) * a_{o_1}*(1-a_{o_1}) * a_{h_1}
$$
针对上述公式，为了表达方便，使用 $\delta_{o_1}$ 来表示输出层的误差：
$$
\delta_{o_1} = {\frac {\partial E_{total}}{\partial a_{o_1}}}*{\frac {\partial a_{o_1}}{\partial z_{o_1}} } = \frac {\partial E_{total}} {\partial z_{o_1}} \\
\delta_{o_1} = -(target_{o_1} - a_{o_1}) * a_{o_1}*(1-a_{o_1})
$$
因此整体损失 $E_{total}$ 对 $w_5$ 的偏导值可以表示为：
$$
\frac {\partial E_{total}}{\partial w_5} = \delta_{o_1}*a_{h_1}
$$
最后我们来更新 $w_5$ 的值：
$$
w_5^+ = w_5 - \eta * \frac {\partial E_{total}} {\partial w_5} = 0.4 - 0.5*0.082167041 = 0.35891648 \qquad \eta: 学习率
$$
同理可更新 $w_6, w_7, w_8$ ：
$$
w_6^+ = 0.408666186 \\
w_7^+ = 0.511301270 \\
w_8^+ = 0.561370121
$$

1. 隐藏层 ---> 隐藏层的权值更新：

计算 $\frac {\partial E_{total}} {\partial w_1}$ 与上述方法类似，但需要注意下图：

![3.png](https://cdn.staticaly.com/gh/TransformersWsz/image_hosting@master/20190518224736113.1umei2jgzokg.webp)

计算 $\frac {\partial E_{total}} {\partial a_{h_1}}$ ：
$$
\frac {\partial E_{total}} {\partial a_{h_1}} = \frac {\partial E_{o_1}} {\partial a_{h_1}} + \frac {\partial E_{o_2}} {\partial a_{h_1}}
$$

先计算 $\frac {\partial E_{o_1}} {\partial a_{h_1}}$ ：

$$
\begin{aligned}
   \frac{\partial E_{o_1}}{\partial a_{h_1}} & =\frac{\partial E_{o_1}}{\partial a_{o_1}} * \frac{\partial a_{o_1}}{\partial z_{o_1}} * \frac{\partial z_{o_1}}{\partial a_{h_1}} \\
   & =0.741365069 * 0.186815602 * 0.4 \\
   & =0.055399425
\end{aligned}
$$


同理可得：
$$
\frac {\partial E_{o_2}} {\partial a_{h_1}} = -0.019049119
$$
两者相加得：
$$
\frac {\partial E_{total}} {\partial a_{h_1}} = 0.055399425 - 0.019049119 = 0.036350306
$$
计算 $\frac {a_{h_1}} {z_{h_1}}$ ：
$$
\frac {a_{h_1}} {z_{h_1}} = a_{h_1} * (1-a_{h_1}) = 0.593269992*(1-0.593269992) = 0.2413007086
$$
计算 $\frac {\partial z_{h_1}} {\partial w_1}$
$$
\frac {\partial z_{h_1}} {\partial w_1} = i_1 = 0.05
$$
最后三者相互乘：
$$
\frac {\partial E_{total}} {\partial w_1} = 0.036350306 * 0.2413007086 * 0.05 = 0.000438568
$$

为了简化公式，用 $\delta_{h_1}$ 表示隐藏层单元 $h_1$ 的误差： 

$$
\begin{aligned}
   \frac{\partial E_{\text {total }}}{\partial w_1} & =\left(\sum_i \frac{\partial E_{\text {total }}}{\partial a_i} * \frac{\partial a_i}{\partial z_i} * \frac{\partial z_i}{\partial h_1}\right) * \frac{\partial a_{h_1}}{\partial z_{h_1}} * \frac{\partial z_{h_1}}{\partial w_1} \\
   & =\left(\sum_i \delta_i * w_{h_i}\right) * a_{h_1} *\left(1-a_{h_1}\right) * i_1 \\
   & =\delta_{h_1} * i_1
\end{aligned}
$$

最后更新 $w_1$ 的权值：
$$
w_1^+ = w_1 - \eta * \frac {\partial E_{total}} {\partial w_1} = 0.15 - 0.5*0.000438568 = 0.149780716
$$
同理，更新 $w_2, w_3, w_4$ 权值：
$$
w_2^+ = 0.19956143 \\
w_3^+ = 0.24975114 \\
w_4^+ = 0.29950229
$$
这样，反向传播算法就完成了，最后我们再把更新的权值重新计算，不停地迭代。在这个例子中第一次迭代之后，总误差 $E_{total}$ 由0.298371109下降至0.291027924。迭代10000次后，总误差为0.000035085，输出为$[0.015912196,0.984065734](原输入为[0.01,0.99]$ ，证明效果还是不错的。

# 公式推导

![4.png](https://cdn.staticaly.com/gh/TransformersWsz/image_hosting@master/20190518225132914.jrimp1wc9o0.webp)

## 符号说明

|      符号      |                             说明                             |
| :------------: | :----------------------------------------------------------: |
|     $n_l$      |                           网络层数                           |
|     $y_j$      |                     输出层第 $j$ 类标签                      |
|     $S_l$      |             第 $l$ 层神经元个数（不包括偏置项）              |
|     $g(x)$     |                           激活函数                           |
|  $w_{ij}^{l}$  | 第 $l-1$ 层的第 $j$ 个神经元连接到第 $l$ 层第 $i$ 个神经元的权重 |
|   $b_i^{l}$    |               第 $l$ 层的第 $i$ 个神经元的偏置               |
|   $z_i^{l}$    |            第 $l$ 层的第 $i$ 个神经元的输入加权和            |
|   $a_i^{l}$    |          第 $l$ 层的第 $i$ 个神经元的输出（激活值）          |
| $\delta_i^{l}$ |             第 $l$ 层的第 $i$ 个神经元产生的错误             |

## 推导过程

### 基本公式

$$
\begin{aligned}
   z_i^l & =\sum_k w_{i k}^l * a_k^{l-1}+b_j^l \\
   g(x) & =\frac{1}{1+e^{-x}} \\
   a_i^l & =g\left(z_i^l\right) \\
   J(\theta) & =\frac{1}{2} \sum_{j=1}^{S_{n l}}\left(y_j-a_j^{n l}\right)^2 \\
   \delta_i^l & =\frac{\partial J(\theta)}{\partial z_i^l}
\end{aligned}
$$

### 梯度方向传播公式推导

#### 初始条件
以一个输入样本为例：
$$
\begin{aligned}
   \delta_i^{n l} & =\frac{\partial J(\theta)}{\partial z_i^{n l}} \\
   & =\frac{1}{2} * \frac{\partial \sum_{j=1}^{S_{n l}}\left(y_j-a_j^{n l}\right)^2}{\partial z_i^{n l}} \\
   & =\frac{1}{2} * \frac{\partial \sum_{j=1}^{S_{n l}}\left(y_j-g\left(z_j^{n l}\right)\right)^2}{\partial z_i^{n l}} \\
   & =\frac{1}{2} \frac{\partial\left(y_i-g\left(z_i^{n l}\right)\right)^2}{\partial z_i^{n l}} \\
   & =-\left(y_i-a_i^{n l}\right) g^{\prime}\left(z_i^{l l}\right)
\end{aligned}
$$



#### 递推公式

$$
\begin{aligned}
   \delta_i^l & =\frac{\partial J(\theta)}{\partial z_i^l} \\
   & =\sum_{j=1}^{S_{l+1}} \frac{\partial J(\theta)}{\partial z_j^{l+1}} * \frac{\partial z_j^{l+1}}{\partial a_i^l} * \frac{\partial a_i^l}{\partial z_i^l} \\
   & =\sum_{j=1}^{S_{l+1}} \frac{\partial J(\theta)}{\partial z_j^{l+1}} * \frac{\partial\left(\sum_k w_{j k}^{l+1} * a_k^l+b_j^{l+1}\right)}{\partial a_i^l} * \frac{\partial a_i^l}{\partial z_i^l} \\
   & =\sum_{j=1}^{S_{l+1}} \delta_j^{l+1} * w_{j i}^{l+1} * g^{\prime}\left(z_i^l\right) \\
   & =g^{\prime}\left(z_i^l\right) * \sum_{j=1}^{S_{l+1}} \delta_j^{l+1} * w_{j i}^{l+1} \\
   \frac{\partial J(\theta)}{\partial w_{i j}^l} & =\frac{\partial J(\theta)}{\partial z_i^l} * \frac{\partial z_i^l}{\partial w_{i j}^l} \\
   & =\delta_i^l * a_j^{l-1} \\
   \frac{\partial J(\theta)}{\partial b_i^l} & =\delta_i^l
\end{aligned}
$$

## 反向传播伪代码

1. 输入训练集。
2. 对于训练集的每个样本 $\vec x$ ，设输入层对应的激活值为 $a^l$ ：
   - 前向传播：$z^l = w^l*a^{l-1}+b^l, a^l = g(z^l)$
   - 计算输出层产生的误差：$\delta^L = \frac {\partial J(\theta)} {\partial a^L} \odot g'(z^L)$
   - 反向传播错误：$\delta^l = ((w^{l+1})^T*\delta^{l+1}) \odot g'(z^l)$
3. 使用梯度下降训练参数：
   - $w^l \dashrightarrow w^l - \frac {\alpha} {m} \sum_x\delta^{x, l}*(a^{x, l-1})^T$
   - $b^l \dashrightarrow  b^l - \frac {\eta} {m} \sum_x\delta^{x, l}$

# 交叉熵损失函数推导

对于多分类问题，$softmax$ 函数可以将神经网络的输出变成一个概率分布。它只是一个额外的处理层，下图展示了加上了 $softmax$ 回归的神经网络结构图：

![softmax](https://img-blog.csdnimg.cn/20190518225454143.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RyYW5zZm9ybWVyX1dTWg==,size_16,color_FFFFFF,t_70)

递推公式仍然和上述递推公式保持一致。初始条件如下：

$$
\begin{aligned}
   J(\theta) & =-\sum_{i=1}^{S_{n l}} y^t * \ln y^p \\
   y_j^p & =\frac{e^{a_j^{n l}}}{\sum_{k=1}^{S_{n l}} e^{a_k^{n l}}}
\end{aligned}
$$

$softmax$ 偏导数计算：
$$
\frac {\partial y_j^p} {\partial a_i^{nl}} =
\left\{
\begin{aligned}
-y_i^p*y_j^p \qquad i \neq j \\
y_i^p*(1-y_i^p) i = j
\end{aligned}
\right.
$$

## 推导过程
$$
\begin{aligned}
   & \frac{\partial J(\theta)}{\partial z_i^{n l}}=\frac{\partial J(\theta)}{\partial a_i^{n l}} * \frac{\partial a_i^{n l}}{\partial z_i^{n l}} \\
   & \frac{\partial J(\theta)}{\partial a_i^{n l}}=\sum_{j=1}^{S_{n l}} \frac{\partial J(\theta)}{\partial y_j^p} * \frac{\partial y_j^p}{\partial a_i^{n l}} \\
   & \frac{\partial J(\theta)}{\partial y_j^p}=-\frac{y_j^t}{y_j^p}
\end{aligned}
   $$
   由上可知:
   $$
\begin{aligned}
   \frac{\partial J(\theta)}{\partial a_i^{n l}} & =\frac{\partial J(\theta)}{\partial y_j^p} * \frac{\partial y_j^p}{\partial a_i^{n l}}+\sum_{j \neq i}^{S_{n t}} \frac{\partial J(\theta)}{\partial y_j^p} * \frac{\partial y_j^p}{\partial a_i^{n l}} \\
   & =-\frac{y_i^t}{y_i^p} * y_i^p *\left(1-y_i^p\right)+\sum_{j \neq i}^{S_{n l}}-\frac{y_j^t}{y_j^p} *\left(-y_j^p * y_i^p\right) \\
   & =-y_i^t+y_i^t * y_i^p+\sum_{j \neq i}^{S_{n l l}}\left(y_j^t * y_i^p\right) \\
   & =-y_i^t+\sum_j^{S_{n+1}}\left(y_j^t * y_i^p\right) \\
   & =y_i^p-y_i^t
\end{aligned}
   $$
   $\therefore$ 反向传播选代算法的初始值为:
   $$
\begin{aligned}
   \frac{\partial J(\theta)}{\partial z_i^{n l}} & =\frac{\partial J(\theta)}{\partial a_i^{n l}} * \frac{\partial a_i^{n l}}{\partial z_i^{n l}} \\
   & =\left(y_i^p-y_i^t\right) * a_i^{n l} *\left(1-a_i^{n l}\right)
\end{aligned}
$$



___

## 参考

- [一文弄懂神经网络中的反向传播法——BackPropagation](https://www.cnblogs.com/charlotte77/p/5629865.html?tdsourcetag=s_pcqq_aiomsg)
- [反向传播算法（过程及公式推导）](https://blog.csdn.net/u014313009/article/details/51039334)
- [反向传播公式推导](https://www.cnblogs.com/nowgood/p/backprop2.html?tdsourcetag=s_pcqq_aiomsg)